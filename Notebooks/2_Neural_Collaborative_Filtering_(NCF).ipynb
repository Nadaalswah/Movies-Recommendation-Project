{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Neural Collaborative Filtering (NCF) is a technique used in recommendation systems that leverages neural networks to learn user-item interactions and make personalized recommendations.<br>\n",
        "NCF enhances CF by employing neural networks to model user-item interactions. It combines the strengths of collaborative filtering with the non-linear capabilities of neural networks, allowing it to capture complex patterns and dependencies in data."
      ],
      "metadata": {
        "id": "CX6SetgDv4As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ACh5FYW1rSG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uuQ_YuyVhTOE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "iOIzlLipnLpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the MovieLens dataset\n",
        "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "dataset_path = '/content/ml-latest-small.zip'\n",
        "\n",
        "# Download and unzip the dataset\n",
        "!wget -nc $url -O $dataset_path\n",
        "!unzip -n $dataset_path -d /content/\n",
        "\n",
        "# Load data into pandas dataframes\n",
        "ratings = pd.read_csv('/content/ml-latest-small/ratings.csv')\n",
        "movies = pd.read_csv('/content/ml-latest-small/movies.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPwOEwEemX9w",
        "outputId": "e54ea813-91eb-448d-f39d-544b760811e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-30 05:51:05--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 978202 (955K) [application/zip]\n",
            "Saving to: ‘/content/ml-latest-small.zip’\n",
            "\n",
            "/content/ml-latest- 100%[===================>] 955.28K   938KB/s    in 1.0s    \n",
            "\n",
            "2024-06-30 05:51:07 (938 KB/s) - ‘/content/ml-latest-small.zip’ saved [978202/978202]\n",
            "\n",
            "Archive:  /content/ml-latest-small.zip\n",
            "   creating: /content/ml-latest-small/\n",
            "  inflating: /content/ml-latest-small/links.csv  \n",
            "  inflating: /content/ml-latest-small/tags.csv  \n",
            "  inflating: /content/ml-latest-small/ratings.csv  \n",
            "  inflating: /content/ml-latest-small/README.txt  \n",
            "  inflating: /content/ml-latest-small/movies.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that user and item IDs are zero-indexed and contiguous\n",
        "user_id_map = {id: idx for idx, id in enumerate(ratings['userId'].unique())}\n",
        "item_id_map = {id: idx for idx, id in enumerate(ratings['movieId'].unique())}\n"
      ],
      "metadata": {
        "id": "p_HJoONNlUBN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings['userId'] = ratings['userId'].map(user_id_map)\n",
        "ratings['movieId'] = ratings['movieId'].map(item_id_map)"
      ],
      "metadata": {
        "id": "xgNvNoz3lap2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the size of the training and testing sets\n",
        "print(f'Training data size: {len(train_data)}')\n",
        "print(f'Testing data size: {len(test_data)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo-qA82-kk38",
        "outputId": "7a1caff5-3354-4caf-c3d4-99a88c0152f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 80668\n",
            "Testing data size: 20168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dataset class\n",
        "class MovieLensDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df['userId'].values, dtype=torch.long)\n",
        "        self.items = torch.tensor(df['movieId'].values, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
        "\n",
        "train_dataset = MovieLensDataset(train_data)\n",
        "test_dataset = MovieLensDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "fv6SeuzpjgM9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative Filtering (NCF)"
      ],
      "metadata": {
        "id": "wDJP2TKzhh8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
        "        super(NCF, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_embedding(user)\n",
        "        item_emb = self.item_embedding(item)\n",
        "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x.squeeze()\n"
      ],
      "metadata": {
        "id": "4igJbmwJjwca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Model, Loss Function, and Optimizer\n",
        "num_users = ratings['userId'].nunique()\n",
        "num_items = ratings['movieId'].nunique()\n",
        "model = NCF(num_users, num_items)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "vPNeoKGzkxh9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the NCF Model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for user, item, rating in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(user, item)\n",
        "        loss = criterion(output, rating)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnTXtOIEk612",
        "outputId": "7a1634eb-39c6-4768-beef-1b11c475b537"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.12443275705665278\n",
            "Epoch 2, Loss: 0.12259010771181639\n",
            "Epoch 3, Loss: 0.11996916712842982\n",
            "Epoch 4, Loss: 0.11682445621804809\n",
            "Epoch 5, Loss: 0.11507377792725858\n",
            "Epoch 6, Loss: 0.11200470077043767\n",
            "Epoch 7, Loss: 0.11110227847505807\n",
            "Epoch 8, Loss: 0.107911557295935\n",
            "Epoch 9, Loss: 0.10616805242532591\n",
            "Epoch 10, Loss: 0.1033319415765372\n",
            "Epoch 11, Loss: 0.10187044237207837\n",
            "Epoch 12, Loss: 0.10047022563765014\n",
            "Epoch 13, Loss: 0.09792401810294102\n",
            "Epoch 14, Loss: 0.09741753001507551\n",
            "Epoch 15, Loss: 0.09463637385998899\n",
            "Epoch 16, Loss: 0.09330134262448261\n",
            "Epoch 17, Loss: 0.09199076949143864\n",
            "Epoch 18, Loss: 0.09035812252741027\n",
            "Epoch 19, Loss: 0.08862357415128898\n",
            "Epoch 20, Loss: 0.08830355685337209\n",
            "Epoch 21, Loss: 0.08611474075693634\n",
            "Epoch 22, Loss: 0.08434879851012736\n",
            "Epoch 23, Loss: 0.08338370667822204\n",
            "Epoch 24, Loss: 0.0826250907952449\n",
            "Epoch 25, Loss: 0.08113884546518516\n",
            "Epoch 26, Loss: 0.08031611471281082\n",
            "Epoch 27, Loss: 0.07912479469982478\n",
            "Epoch 28, Loss: 0.07801626555763427\n",
            "Epoch 29, Loss: 0.07744832704852585\n",
            "Epoch 30, Loss: 0.07622944913704087\n",
            "Epoch 31, Loss: 0.07504823740681044\n",
            "Epoch 32, Loss: 0.07429507584218749\n",
            "Epoch 33, Loss: 0.07289427958730192\n",
            "Epoch 34, Loss: 0.07226433645724116\n",
            "Epoch 35, Loss: 0.07174099878977444\n",
            "Epoch 36, Loss: 0.07034420390287673\n",
            "Epoch 37, Loss: 0.07003336122981808\n",
            "Epoch 38, Loss: 0.06831819535743425\n",
            "Epoch 39, Loss: 0.06811993707098439\n",
            "Epoch 40, Loss: 0.06780431871165793\n",
            "Epoch 41, Loss: 0.06600384639899566\n",
            "Epoch 42, Loss: 0.06534782586313495\n",
            "Epoch 43, Loss: 0.0648849106066573\n",
            "Epoch 44, Loss: 0.06422275213508451\n",
            "Epoch 45, Loss: 0.06346163050102441\n",
            "Epoch 46, Loss: 0.06307856601047715\n",
            "Epoch 47, Loss: 0.06232524202792395\n",
            "Epoch 48, Loss: 0.061529471133492185\n",
            "Epoch 49, Loss: 0.06149266221952712\n",
            "Epoch 50, Loss: 0.060436888455363796\n",
            "Epoch 51, Loss: 0.05991581469496548\n",
            "Epoch 52, Loss: 0.05899818609693899\n",
            "Epoch 53, Loss: 0.05866690032763154\n",
            "Epoch 54, Loss: 0.058177471103281087\n",
            "Epoch 55, Loss: 0.057177346202705134\n",
            "Epoch 56, Loss: 0.05726092170501493\n",
            "Epoch 57, Loss: 0.056394482538115874\n",
            "Epoch 58, Loss: 0.05600438764928874\n",
            "Epoch 59, Loss: 0.05569851907482703\n",
            "Epoch 60, Loss: 0.05522631134462678\n",
            "Epoch 61, Loss: 0.05411570416696954\n",
            "Epoch 62, Loss: 0.054169780681829996\n",
            "Epoch 63, Loss: 0.05320754706635067\n",
            "Epoch 64, Loss: 0.053029069041834756\n",
            "Epoch 65, Loss: 0.05257526009353166\n",
            "Epoch 66, Loss: 0.051891776234573264\n",
            "Epoch 67, Loss: 0.05153436046669778\n",
            "Epoch 68, Loss: 0.05117089681114638\n",
            "Epoch 69, Loss: 0.05037628564005755\n",
            "Epoch 70, Loss: 0.05049413248961592\n",
            "Epoch 71, Loss: 0.04960094433363518\n",
            "Epoch 72, Loss: 0.04956801022084623\n",
            "Epoch 73, Loss: 0.048974935118992306\n",
            "Epoch 74, Loss: 0.04886165838082994\n",
            "Epoch 75, Loss: 0.04843513628587564\n",
            "Epoch 76, Loss: 0.047597529332638736\n",
            "Epoch 77, Loss: 0.047409459704815346\n",
            "Epoch 78, Loss: 0.04740831348971182\n",
            "Epoch 79, Loss: 0.04712560109374638\n",
            "Epoch 80, Loss: 0.04667957126503422\n",
            "Epoch 81, Loss: 0.046441501056706025\n",
            "Epoch 82, Loss: 0.04585629634110111\n",
            "Epoch 83, Loss: 0.04587820245574903\n",
            "Epoch 84, Loss: 0.04523881499690257\n",
            "Epoch 85, Loss: 0.04500454959506132\n",
            "Epoch 86, Loss: 0.04484192851390232\n",
            "Epoch 87, Loss: 0.04463146352574737\n",
            "Epoch 88, Loss: 0.04406253773125271\n",
            "Epoch 89, Loss: 0.04391736275177452\n",
            "Epoch 90, Loss: 0.043419556554870696\n",
            "Epoch 91, Loss: 0.043301554260853733\n",
            "Epoch 92, Loss: 0.04316552894442541\n",
            "Epoch 93, Loss: 0.042394791556186945\n",
            "Epoch 94, Loss: 0.04256987333014123\n",
            "Epoch 95, Loss: 0.04245606662290471\n",
            "Epoch 96, Loss: 0.04204020970910906\n",
            "Epoch 97, Loss: 0.0420188264423519\n",
            "Epoch 98, Loss: 0.0407168870152201\n",
            "Epoch 99, Loss: 0.041174659465604314\n",
            "Epoch 100, Loss: 0.04081550556324381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists for predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Set a threshold for relevance\n",
        "threshold = 4.0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for user, item, rating in test_loader:\n",
        "        output = model(user, item)\n",
        "        all_preds.extend(output.numpy())\n",
        "        all_labels.extend(rating.numpy())\n",
        "\n",
        "# Convert predictions to binary relevance (rating >= threshold is relevant)\n",
        "all_preds_binary = [1 if pred >= threshold else 0 for pred in all_preds]\n",
        "all_labels_binary = [1 if label >= threshold else 0 for label in all_labels]\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = sum(1 for pred, label in zip(all_preds_binary, all_labels_binary) if pred == label)\n",
        "accuracy = correct / len(all_preds_binary)\n",
        "\n",
        "print(f'NCF Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvQN-G5KsVZw",
        "outputId": "7a5069e5-bc45-4333-8a09-ff1dff752274"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCF Test Accuracy: 0.6260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'ncf_model.pth')"
      ],
      "metadata": {
        "id": "h4xeyqotmC-h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------"
      ],
      "metadata": {
        "id": "wD_b2sXcmuHT"
      }
    }
  ]
}